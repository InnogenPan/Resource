{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import megengine\n",
    "import megengine.data as data\n",
    "import megengine.data.transform as T\n",
    "import megengine.module as M\n",
    "import megengine.functional as F\n",
    "import megengine.optimizer as optimizer\n",
    "import megengine.jit as jit\n",
    "import megengine.distributed as dist\n",
    "import multiprocessing as mp\n",
    "\n",
    "F.sync_batch_norm\n",
    "M.SyncBatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(M.Module):\n",
    "    \"\"\"每个ResNet18的Block都包含两层卷积\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # 第一层卷积，接 BN 和 ReLU\n",
    "        self.conv1 = M.ConvBnRelu2d(\n",
    "            in_channels=in_channels, out_channels=out_channels,\n",
    "            kernel_size=3, stride=stride, padding=1)\n",
    "        # 第二层卷积，只接 BN\n",
    "        self.conv2 = M.ConvBn2d(\n",
    "            in_channels=out_channels, out_channels=out_channels,\n",
    "            kernel_size=3, stride=1, padding=1)\n",
    "        # 残差连接，当输入输出不一致/需要下采样时，用 ConvBn 实现变换\n",
    "        if in_channels == out_channels and stride == 1:\n",
    "            self.res_conn = M.Identity()\n",
    "        else:\n",
    "            self.res_conn = M.ConvBn2d(\n",
    "                in_channels=in_channels, out_channels=out_channels,\n",
    "                kernel_size=1, stride=stride)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x + self.res_conn(identity)\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(M.Module):\n",
    "    def __init__(self):\n",
    "        self.conv1 = M.ConvBnRelu2d(in_channels=3, out_channels=64,\n",
    "                                    kernel_size=3, padding=1)\n",
    "        # 8 个 BasicBlock，3 次下采样(stride=2)，共 8x2=16 层卷积\n",
    "        self.blocks = M.Sequential(\n",
    "            BasicBlock(64,  64),\n",
    "            BasicBlock(64,  64),\n",
    "            BasicBlock(64,  128, stride=2),\n",
    "            BasicBlock(128, 128),\n",
    "            BasicBlock(128, 256, stride=2),\n",
    "            BasicBlock(256, 256),\n",
    "            BasicBlock(256, 512, stride=2),\n",
    "            BasicBlock(512, 512),\n",
    "        )\n",
    "        # 全连接分类器，输出维度为 10 类的预测\n",
    "        self.classifier = M.Sequential(\n",
    "            M.Dropout(0.2),\n",
    "            M.Linear(512, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 1. 特征提取，输入为 Nx3x32x32 的图片，输出为 Nx512x4x4的张量(Tensor)\n",
    "        x = self.conv1(x)\n",
    "        x = self.blocks(x)\n",
    "        # 2. 4x4平均池化(Average Pooling)\n",
    "        x = F.avg_pool2d(x, 4)\n",
    "        x = F.flatten(x, 1)\n",
    "        # 3. 分类预测\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    # megengine内置CIFAR10的数据集\n",
    "    dataset = data.dataset.CIFAR10(root=\"/data\", train=True)\n",
    "    \n",
    "    # 构造数据生产线\n",
    "    dataloader = data.DataLoader(\n",
    "        dataset,\n",
    "        sampler=data.RandomSampler(dataset, batch_size=64, drop_last=True),\n",
    "        transform=T.Compose([\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.Normalize(mean=0., std=255.),  # f(x) = (x - mean) / std\n",
    "            T.ToMode(\"CHW\"),\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # 构造网络与输入\n",
    "    model = ResNet18()\n",
    "    image = megengine.tensor(dtype=\"float32\")\n",
    "    label = megengine.tensor(dtype=\"int32\")\n",
    "    \n",
    "    # 构造网络优化器\n",
    "    opt = optimizer.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=1e-4)\n",
    "    \n",
    "    # 构造静态的计算图以充分发挥性能\n",
    "    @jit.trace\n",
    "    def train_func(image, label):\n",
    "        # 前传\n",
    "        loglikelihood = model(image)\n",
    "        loss = F.cross_entropy_with_softmax(loglikelihood, label)\n",
    "        accuracy = F.accuracy(loglikelihood, label)\n",
    "\n",
    "        # 反传并更新网络参数\n",
    "        opt.zero_grad()\n",
    "        opt.backward(loss)\n",
    "        opt.step()\n",
    "        return loss, accuracy\n",
    "        \n",
    "    for epoch in range(90):\n",
    "        # 训练一个epoch == 遍历一次训练数据集\n",
    "        for i, batch_data in enumerate(dataloader):\n",
    "            # 进行一次迭代\n",
    "            image.set_value(batch_data[0])\n",
    "            label.set_value(batch_data[1])\n",
    "            \n",
    "            loss, acc1 = train_func(image, label)\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                print(\"epoch\", epoch, \"step\", i, \"loss\", loss, \"acc@1\", acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(rank):\n",
    "    # 每个子进程需要初始化分布式进程组\n",
    "    dist.init_process_group(\n",
    "        master_ip=\"localhost\",  # 主节点的IP地址。单机多卡的情况下可以简单设为localhost\n",
    "        master_port=2233,       # 进行通信的可用的端口号，0-65535，注意不能被其他进程占用\n",
    "        world_size=8,           # 参与任务的进程总数（即总进程个数，也等于显卡个数）\n",
    "        rank=rank,              # 当前进程的进程号（即第几个进程）\n",
    "        dev=rank,               # 第几个进程用第几块显卡\n",
    "    )\n",
    "    print(\"init process\", rank)\n",
    "    # 开始训练\n",
    "    training()\n",
    "\n",
    "def main():\n",
    "    # 在一台机器上启动 8 个子进程（因为总共有 8 块显卡）\n",
    "    # 关于 multiprocessing 的使用方法参见python官方文档\n",
    "    for rank in range(8):\n",
    "        p = mp.Process(target=worker, args=(rank,))\n",
    "        p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init process 0\n",
      "init process 1\n",
      "init process 2\n",
      "init process 3\n",
      "init process 4\n",
      "init process 5\n",
      "init process 6\n",
      "init process 7\n",
      "epoch 0 step 0 loss Tensor([2.5735], device=gpu6:0) acc@1 Tensor([0.0781], device=gpu6:0)\n",
      "epoch 0 step 0 loss Tensor([2.4922], device=gpu1:0) acc@1 Tensor([0.0625], device=gpu1:0)\n",
      "epoch 0 step 0 loss Tensor([2.5264], device=gpu3:0) acc@1 Tensor([0.1406], device=gpu3:0)\n",
      "epoch 0 step 0 loss Tensor([2.7726], device=gpu5:0) acc@1 Tensor([0.], device=gpu5:0)\n",
      "epoch 0 step 0 loss Tensor([2.6011], device=gpu2:0) acc@1 Tensor([0.0938], device=gpu2:0)\n",
      "epoch 0 step 0 loss Tensor([2.5178], device=gpu7:0) acc@1 Tensor([0.0312], device=gpu7:0)\n",
      "epoch 0 step 0 loss Tensor([2.554], device=gpu4:0) acc@1 Tensor([0.1094], device=gpu4:0)\n",
      "epoch 0 step 0 loss Tensor([2.5615], device=gpu0:0) acc@1 Tensor([0.1406], device=gpu0:0)\n",
      "epoch 0 step 50 loss Tensor([1.7417], device=gpu6:0) acc@1 Tensor([0.4219], device=gpu6:0)\n",
      "epoch 0 step 50 loss Tensor([1.521], device=gpu5:0) acc@1 Tensor([0.5312], device=gpu5:0)\n",
      "epoch 0 step 50 loss Tensor([1.6334], device=gpu1:0) acc@1 Tensor([0.3281], device=gpu1:0)\n",
      "epoch 0 step 50 loss Tensor([1.6283], device=gpu4:0) acc@1 Tensor([0.375], device=gpu4:0)\n",
      "epoch 0 step 50 loss Tensor([1.5762], device=gpu2:0) acc@1 Tensor([0.375], device=gpu2:0)\n",
      "epoch 0 step 50 loss Tensor([1.5677], device=gpu0:0) acc@1 Tensor([0.4062], device=gpu0:0)\n",
      "epoch 0 step 50 loss Tensor([1.6616], device=gpu3:0) acc@1 Tensor([0.3594], device=gpu3:0)\n",
      "epoch 0 step 50 loss Tensor([1.6297], device=gpu7:0) acc@1 Tensor([0.4531], device=gpu7:0)\n",
      "epoch 1 step 0 loss Tensor([1.2938], device=gpu7:0) acc@1 Tensor([0.5], device=gpu7:0)\n",
      "epoch 1 step 0 loss Tensor([1.5492], device=gpu1:0) acc@1 Tensor([0.4219], device=gpu1:0)\n",
      "epoch 1 step 0 loss Tensor([1.3215], device=gpu0:0) acc@1 Tensor([0.4531], device=gpu0:0)\n",
      "epoch 1 step 0 loss Tensor([1.5797], device=gpu5:0) acc@1 Tensor([0.5], device=gpu5:0)\n",
      "epoch 1 step 0 loss Tensor([1.5253], device=gpu6:0) acc@1 Tensor([0.4219], device=gpu6:0)\n",
      "epoch 1 step 0 loss Tensor([1.5287], device=gpu4:0) acc@1 Tensor([0.375], device=gpu4:0)\n",
      "epoch 1 step 0 loss Tensor([1.4993], device=gpu3:0) acc@1 Tensor([0.4062], device=gpu3:0)\n",
      "epoch 1 step 0 loss Tensor([1.5306], device=gpu2:0) acc@1 Tensor([0.4219], device=gpu2:0)\n",
      "epoch 1 step 50 loss Tensor([1.0871], device=gpu6:0) acc@1 Tensor([0.5938], device=gpu6:0)\n",
      "epoch 1 step 50 loss Tensor([1.2019], device=gpu5:0) acc@1 Tensor([0.5625], device=gpu5:0)\n",
      "epoch 1 step 50 loss Tensor([1.449], device=gpu1:0) acc@1 Tensor([0.5], device=gpu1:0)\n",
      "epoch 1 step 50 loss Tensor([1.3241], device=gpu2:0) acc@1 Tensor([0.5625], device=gpu2:0)\n",
      "epoch 1 step 50 loss Tensor([1.3581], device=gpu7:0) acc@1 Tensor([0.4531], device=gpu7:0)\n",
      "epoch 1 step 50 loss Tensor([1.3815], device=gpu3:0) acc@1 Tensor([0.5], device=gpu3:0)\n",
      "epoch 1 step 50 loss Tensor([1.5198], device=gpu4:0) acc@1 Tensor([0.375], device=gpu4:0)\n",
      "epoch 1 step 50 loss Tensor([1.2055], device=gpu0:0) acc@1 Tensor([0.5156], device=gpu0:0)\n",
      "epoch 2 step 0 loss Tensor([1.313], device=gpu1:0) acc@1 Tensor([0.5156], device=gpu1:0)\n",
      "epoch 2 step 0 loss Tensor([1.0568], device=gpu6:0) acc@1 Tensor([0.6094], device=gpu6:0)\n",
      "epoch 2 step 0 loss Tensor([1.4929], device=gpu5:0) acc@1 Tensor([0.5625], device=gpu5:0)\n",
      "epoch 2 step 0 loss Tensor([1.1126], device=gpu7:0) acc@1 Tensor([0.5938], device=gpu7:0)\n",
      "epoch 2 step 0 loss Tensor([1.1693], device=gpu4:0) acc@1 Tensor([0.5781], device=gpu4:0)\n",
      "epoch 2 step 0 loss Tensor([1.1453], device=gpu2:0) acc@1 Tensor([0.5312], device=gpu2:0)\n",
      "epoch 2 step 0 loss Tensor([1.235], device=gpu0:0) acc@1 Tensor([0.4844], device=gpu0:0)\n",
      "epoch 2 step 0 loss Tensor([1.1699], device=gpu3:0) acc@1 Tensor([0.5156], device=gpu3:0)\n",
      "epoch 2 step 50 loss Tensor([1.3884], device=gpu6:0) acc@1 Tensor([0.5], device=gpu6:0)\n",
      "epoch 2 step 50 loss Tensor([0.9665], device=gpu7:0) acc@1 Tensor([0.5938], device=gpu7:0)\n",
      "epoch 2 step 50 loss Tensor([1.0038], device=gpu1:0) acc@1 Tensor([0.5938], device=gpu1:0)\n",
      "epoch 2 step 50 loss Tensor([1.1243], device=gpu0:0) acc@1 Tensor([0.5781], device=gpu0:0)\n",
      "epoch 2 step 50 loss Tensor([1.014], device=gpu5:0) acc@1 Tensor([0.7188], device=gpu5:0)\n",
      "epoch 2 step 50 loss Tensor([1.077], device=gpu4:0) acc@1 Tensor([0.5156], device=gpu4:0)\n",
      "epoch 2 step 50 loss Tensor([1.1881], device=gpu2:0) acc@1 Tensor([0.6562], device=gpu2:0)\n",
      "epoch 2 step 50 loss Tensor([1.0908], device=gpu3:0) acc@1 Tensor([0.5312], device=gpu3:0)\n",
      "epoch 3 step 0 loss Tensor([1.1245], device=gpu1:0) acc@1 Tensor([0.5625], device=gpu1:0)\n",
      "epoch 3 step 0 loss Tensor([1.0347], device=gpu5:0) acc@1 Tensor([0.6875], device=gpu5:0)\n",
      "epoch 3 step 0 loss Tensor([0.9063], device=gpu7:0) acc@1 Tensor([0.6875], device=gpu7:0)\n",
      "epoch 3 step 0 loss Tensor([1.046], device=gpu3:0) acc@1 Tensor([0.6562], device=gpu3:0)\n",
      "epoch 3 step 0 loss Tensor([0.961], device=gpu6:0) acc@1 Tensor([0.7031], device=gpu6:0)\n",
      "epoch 3 step 0 loss Tensor([0.7993], device=gpu0:0) acc@1 Tensor([0.7188], device=gpu0:0)\n",
      "epoch 3 step 0 loss Tensor([0.8532], device=gpu2:0) acc@1 Tensor([0.6406], device=gpu2:0)\n",
      "epoch 3 step 0 loss Tensor([1.0973], device=gpu4:0) acc@1 Tensor([0.6094], device=gpu4:0)\n",
      "epoch 3 step 50 loss Tensor([0.8748], device=gpu7:0) acc@1 Tensor([0.7344], device=gpu7:0)\n",
      "epoch 3 step 50 loss Tensor([0.9524], device=gpu6:0) acc@1 Tensor([0.6875], device=gpu6:0)\n",
      "epoch 3 step 50 loss Tensor([1.0182], device=gpu1:0) acc@1 Tensor([0.7031], device=gpu1:0)\n",
      "epoch 3 step 50 loss Tensor([0.9418], device=gpu4:0) acc@1 Tensor([0.7188], device=gpu4:0)\n",
      "epoch 3 step 50 loss Tensor([0.9911], device=gpu2:0) acc@1 Tensor([0.6719], device=gpu2:0)\n",
      "epoch 3 step 50 loss Tensor([1.0507], device=gpu5:0) acc@1 Tensor([0.6562], device=gpu5:0)\n",
      "epoch 3 step 50 loss Tensor([0.9237], device=gpu3:0) acc@1 Tensor([0.6562], device=gpu3:0)\n",
      "epoch 3 step 50 loss Tensor([0.722], device=gpu0:0) acc@1 Tensor([0.8281], device=gpu0:0)\n",
      "epoch 4 step 0 loss Tensor([0.8674], device=gpu7:0) acc@1 Tensor([0.7188], device=gpu7:0)\n",
      "epoch 4 step 0 loss Tensor([1.0817], device=gpu5:0) acc@1 Tensor([0.5938], device=gpu5:0)\n",
      "epoch 4 step 0 loss Tensor([1.0578], device=gpu1:0) acc@1 Tensor([0.6094], device=gpu1:0)\n",
      "epoch 4 step 0 loss Tensor([0.8786], device=gpu6:0) acc@1 Tensor([0.7031], device=gpu6:0)\n",
      "epoch 4 step 0 loss Tensor([1.2348], device=gpu2:0) acc@1 Tensor([0.5156], device=gpu2:0)\n",
      "epoch 4 step 0 loss Tensor([0.9356], device=gpu4:0) acc@1 Tensor([0.7188], device=gpu4:0)\n",
      "epoch 4 step 0 loss Tensor([0.933], device=gpu0:0) acc@1 Tensor([0.6562], device=gpu0:0)\n",
      "epoch 4 step 0 loss Tensor([0.7652], device=gpu3:0) acc@1 Tensor([0.7188], device=gpu3:0)\n",
      "epoch 4 step 50 loss Tensor([0.7796], device=gpu6:0) acc@1 Tensor([0.7344], device=gpu6:0)\n",
      "epoch 4 step 50 loss Tensor([0.8349], device=gpu5:0) acc@1 Tensor([0.7031], device=gpu5:0)\n",
      "epoch 4 step 50 loss Tensor([0.9181], device=gpu4:0) acc@1 Tensor([0.7344], device=gpu4:0)\n",
      "epoch 4 step 50 loss Tensor([0.9559], device=gpu3:0) acc@1 Tensor([0.5781], device=gpu3:0)\n",
      "epoch 4 step 50 loss Tensor([0.5912], device=gpu2:0) acc@1 Tensor([0.8281], device=gpu2:0)\n",
      "epoch 4 step 50 loss Tensor([0.9764], device=gpu0:0) acc@1 Tensor([0.6562], device=gpu0:0)\n",
      "epoch 4 step 50 loss Tensor([0.9125], device=gpu7:0) acc@1 Tensor([0.6875], device=gpu7:0)\n",
      "epoch 4 step 50 loss Tensor([0.7488], device=gpu1:0) acc@1 Tensor([0.7969], device=gpu1:0)\n",
      "epoch 5 step 0 loss Tensor([0.5704], device=gpu5:0) acc@1 Tensor([0.7812], device=gpu5:0)\n",
      "epoch 5 step 0 loss Tensor([0.9098], device=gpu7:0) acc@1 Tensor([0.6719], device=gpu7:0)\n",
      "epoch 5 step 0 loss Tensor([0.9084], device=gpu2:0) acc@1 Tensor([0.7188], device=gpu2:0)\n",
      "epoch 5 step 0 loss Tensor([0.8209], device=gpu3:0) acc@1 Tensor([0.7188], device=gpu3:0)\n",
      "epoch 5 step 0 loss Tensor([0.8147], device=gpu4:0) acc@1 Tensor([0.6719], device=gpu4:0)\n",
      "epoch 5 step 0 loss Tensor([0.8523], device=gpu6:0) acc@1 Tensor([0.7031], device=gpu6:0)\n",
      "epoch 5 step 0 loss Tensor([0.9633], device=gpu0:0) acc@1 Tensor([0.7031], device=gpu0:0)\n",
      "epoch 5 step 0 loss Tensor([1.0272], device=gpu1:0) acc@1 Tensor([0.6406], device=gpu1:0)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "8GPU",
   "language": "python3",
   "name": "8gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
